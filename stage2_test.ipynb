{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e03162f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "# from networks.vision_transformer import SwinUnet as ViT_seg\n",
    "\n",
    "# from utils import DiceLoss\n",
    "# from config import get_config\n",
    "# from WG_dataset2 import PosNegDataset\n",
    "from dataset import USDataset\n",
    "\n",
    "import csv\n",
    "# from model.encoder import DualImageContrastiveModel\n",
    "# from model.accVoice_swinUnet import SwinTransformerSys as SwinUnet\n",
    "# from model.CrossViT3 import ImageCrossViT\n",
    "from model.stage2_model1 import Stage2Model\n",
    "# from model.AccSeal_model_stage1 import AccSealModelStage1\n",
    "# import networks.accVoice_vitUnet as VitUnet\n",
    "from utils.loss import InfoNCELoss2, InfoNCELoss, BatchInfoNCELoss, FocalLoss\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#single GPU\n",
    "ngpu = 1\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "#multi GPU\n",
    "gpus = [0,1]\n",
    "torch.cuda.set_device('cuda:{}'.format(gpus[0]))\n",
    "\n",
    "\n",
    "def test_model(model):\n",
    "    \n",
    "    batch_size = 384\n",
    "\n",
    "    #test dataset path\n",
    "    #正样本和负样本，负样本是正样本的10倍数量，合在一起进行推理\n",
    "    # data_path = r\"Z:\\dataset\\accelerometer_audio\\AccSeal\\test_dataset_true.csv\" #测试正样本\n",
    "    # data_path = r\"Z:\\dataset\\accelerometer_audio\\AccSeal\\test_dataset_false.csv\" #测试负样本\n",
    "    data_path = r\"E:\\dataset\\ultrasound_video_audio\\DATA\\dataset\\test_dataset.csv\" #负样本\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    test_dataset = USDataset(data_path)\n",
    "    print(\"====== Test Dataset Count: =======\", test_dataset.__len__())\n",
    "\n",
    "\n",
    "    #output path\n",
    "    # output_path = r\"Z:\\dataset\\accelerometer_audio\\AccSeal\\WG\\test_dataset_crossUser_false\" #negitive samples\n",
    "    # output_path = r\"Z:\\dataset\\accelerometer_audio\\AccSeal\\WG\\test_dataset_crossUser_true\" #positive samples\n",
    "    # output_path = r\"Z:\\dataset\\accelerometer_audio\\AccSeal\\WG\\stage2_2_test_dataset_true1\" #\n",
    "    output_path = r\"E:\\dataset\\ultrasound_video_audio\\record\\stage2_test_dataset\" #mismatching negitive samples, 只保存预测的label\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    # 加载预训练模型参数\n",
    "    # model_path = \"./checkpoints/model_epoch_400.pth\"  # 使用最后一个epoch的模型\n",
    "    model_path = \"./checkpoints/stage2/best_model.pth\"  # 使用best_model\n",
    "    # model_path = \"Z:/dataset/accelerometer/record/RESULT_original_xyzTrans_removeTimbre_average_BIGVGAN_swinUnet_LGDZN/model_epoch_400.pth\"  # 使用best_model\n",
    "    # model_path = r\"Z:\\dataset\\accelerometer_audio\\AccVoice\\record\\RESULT_original_removeTimbre_average_BIGVGAN_swinUnet_crossUser2\\best_model.pth\"\n",
    "\n",
    "    model = model.to(device) #single gpu test\n",
    "    # model = nn.DataParallel(model.cuda(), device_ids=gpus, output_device=gpus[0]) #multi gpu test\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    sample_paths = [test_dataset.get_audio_path(i) for i in range(len(test_dataset))] #读取原始音频路径\n",
    "    sample_names = [test_dataset.get_audio_name(i) for i in range(len(test_dataset))]\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        save_path_index = 0\n",
    "        for batch in test_loader:\n",
    "            ultrasound = batch['ultrasound']    #[N, 1, 80，500] batch_size, \n",
    "            video = batch['video']\n",
    "            # speaker = batch['speaker']\n",
    "            # user = batch['user']\n",
    "\n",
    "            # audio = torch.transpose(audio, -2, -1)  #转置，变成[N, 1, 500, 80]\n",
    "            # acc = torch.transpose(acc, -2, -1)\n",
    "            # audio = audio.to(device=device, dtype=torch.float32)  #GPU上训练，数据必须放入GPU，参数在GPU上更新\n",
    "            # label = label.to(device=device, dtype=torch.float32)\n",
    "            modal_video = video.cuda(non_blocking=True)  #GPU上训练，数据必须放入GPU，参数在GPU上更新, non_blocking=True启用异步数据传输。\n",
    "            modal_ultrasound = ultrasound.cuda(non_blocking=True) #将 Tensor 数据从 CPU 移动到 GPU 时，可以通过设置 non_blocking=True 来启用异步传输模式，从而提高计算效率。\n",
    "            \n",
    "            time_start = time.time()\n",
    "            speaker_feature, user_verification = model(modal_ultrasound, modal_video)\n",
    "            # fake_clean = model(noisy_imgs)\n",
    "            time_end = time.time()\n",
    "\n",
    "            print(\"inference time: \", time_end - time_start)\n",
    "            print(\"average inference time per sample: \", (time_end - time_start)/speaker_feature.shape[0])\n",
    "\n",
    "\n",
    "            output = speaker_feature.squeeze(dim=1).cpu().numpy()\n",
    "            output2 = user_verification.squeeze(dim=1).cpu().numpy()\n",
    "            \n",
    "            print(\"output shape: \", output.shape)\n",
    "            for j in range(output.shape[0]):\n",
    "                if save_path_index >= len(sample_names):  # Check if we've reached the end of the dataset\n",
    "                    break\n",
    "                print(f\"Processing sample {save_path_index + 1}/{len(sample_names)}: {sample_names[save_path_index]}\")\n",
    "           \n",
    "                parts = sample_paths[save_path_index].split('\\\\')\n",
    "                # folder1 = parts[-3]\n",
    "                folder2 = parts[-2]\n",
    "     \n",
    "                # Create nested directory structure\n",
    "                # nested_dir = os.path.join(output_path, folder1, folder2)\n",
    "                nested_dir = os.path.join(output_path, folder2)\n",
    "                os.makedirs(nested_dir, exist_ok=True)\n",
    "\n",
    "                # Full output path with nested directories\n",
    "                output_file = os.path.join(nested_dir, f\"{sample_names[save_path_index]}.npy\")\n",
    "                output_file2 = os.path.join(nested_dir, f\"{sample_names[save_path_index]}.txt\")\n",
    "\n",
    "\n",
    "                out = output[j] #存储speaker_feature特征\n",
    "                out2 = output2[j]\n",
    "                # out2 = 0 if out2 < 0.5 else 1\n",
    "\n",
    "                np.save(output_file, out)\n",
    "                with open(output_file2, \"w\") as file:\n",
    "                    file.write(str(out2))\n",
    "                # np.save(output_file2, out2)\n",
    "\n",
    "                save_path_index += 1\n",
    "    # del model, dataloader\n",
    "    # gc.collect()\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "def main_test_CrossViT():\n",
    "    # 创建模型\n",
    "    T = 5\n",
    "    batch_size = 4\n",
    "    feature_dim = 512\n",
    "    frames_per_sec = 10\n",
    "    \n",
    "    # Initialize model\n",
    "    model = Stage2Model(feature_dim=feature_dim)\n",
    "    test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e18eee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset path:  E:\\dataset\\ultrasound_video_audio\\DATA\\dataset\\test_dataset.csv\n",
      "====== Test Dataset Count: ======= 152\n",
      "inference time:  0.5846295356750488\n",
      "average inference time per sample:  0.0038462469452305846\n",
      "output shape:  (152, 50, 512)\n",
      "Processing sample 1/152: db_seg_0149_us\n",
      "Processing sample 2/152: whh_seg_0257_us\n",
      "Processing sample 3/152: syk_seg_0110_us\n",
      "Processing sample 4/152: db_seg_0048_us\n",
      "Processing sample 5/152: whh_seg_0145_us\n",
      "Processing sample 6/152: pmz_seg_0290_us\n",
      "Processing sample 7/152: whh_seg_0219_us\n",
      "Processing sample 8/152: ldx_seg_0051_us\n",
      "Processing sample 9/152: whh_seg_0272_us\n",
      "Processing sample 10/152: whh_seg_0286_us\n",
      "Processing sample 11/152: db_seg_0257_us\n",
      "Processing sample 12/152: zyc_seg_0033_us\n",
      "Processing sample 13/152: ldx_seg_0060_us\n",
      "Processing sample 14/152: whh_seg_0170_us\n",
      "Processing sample 15/152: whh_seg_0107_us\n",
      "Processing sample 16/152: whh_seg_0264_us\n",
      "Processing sample 17/152: syk_seg_0206_us\n",
      "Processing sample 18/152: syk_seg_0055_us\n",
      "Processing sample 19/152: syk_seg_0006_us\n",
      "Processing sample 20/152: whh_seg_0296_us\n",
      "Processing sample 21/152: syk_seg_0201_us\n",
      "Processing sample 22/152: whh_seg_0160_us\n",
      "Processing sample 23/152: syk_seg_0126_us\n",
      "Processing sample 24/152: pmz_seg_0283_us\n",
      "Processing sample 25/152: syk_seg_0013_us\n",
      "Processing sample 26/152: pmz_seg_0185_us\n",
      "Processing sample 27/152: ldx_seg_0261_us\n",
      "Processing sample 28/152: whh_seg_0080_us\n",
      "Processing sample 29/152: pmz_seg_0068_us\n",
      "Processing sample 30/152: whh_seg_0191_us\n",
      "Processing sample 31/152: whh_seg_0032_us\n",
      "Processing sample 32/152: syk_seg_0215_us\n",
      "Processing sample 33/152: pmz_seg_0222_us\n",
      "Processing sample 34/152: ldx_seg_0057_us\n",
      "Processing sample 35/152: db_seg_0087_us\n",
      "Processing sample 36/152: zyc_seg_0121_us\n",
      "Processing sample 37/152: syk_seg_0252_us\n",
      "Processing sample 38/152: syk_seg_0014_us\n",
      "Processing sample 39/152: ldx_seg_0085_us\n",
      "Processing sample 40/152: syk_seg_0273_us\n",
      "Processing sample 41/152: pmz_seg_0051_us\n",
      "Processing sample 42/152: whh_seg_0000_us\n",
      "Processing sample 43/152: whh_seg_0211_us\n",
      "Processing sample 44/152: pmz_seg_0180_us\n",
      "Processing sample 45/152: pmz_seg_0205_us\n",
      "Processing sample 46/152: db_seg_0106_us\n",
      "Processing sample 47/152: pmz_seg_0108_us\n",
      "Processing sample 48/152: ldx_seg_0262_us\n",
      "Processing sample 49/152: pmz_seg_0024_us\n",
      "Processing sample 50/152: whh_seg_0068_us\n",
      "Processing sample 51/152: pmz_seg_0056_us\n",
      "Processing sample 52/152: zyc_seg_0239_us\n",
      "Processing sample 53/152: pmz_seg_0214_us\n",
      "Processing sample 54/152: whh_seg_0028_us\n",
      "Processing sample 55/152: pmz_seg_0216_us\n",
      "Processing sample 56/152: whh_seg_0189_us\n",
      "Processing sample 57/152: db_seg_0102_us\n",
      "Processing sample 58/152: db_seg_0189_us\n",
      "Processing sample 59/152: syk_seg_0287_us\n",
      "Processing sample 60/152: syk_seg_0027_us\n",
      "Processing sample 61/152: pmz_seg_0132_us\n",
      "Processing sample 62/152: ldx_seg_0137_us\n",
      "Processing sample 63/152: whh_seg_0124_us\n",
      "Processing sample 64/152: ldx_seg_0210_us\n",
      "Processing sample 65/152: zyc_seg_0164_us\n",
      "Processing sample 66/152: pmz_seg_0232_us\n",
      "Processing sample 67/152: ldx_seg_0083_us\n",
      "Processing sample 68/152: db_seg_0021_us\n",
      "Processing sample 69/152: db_seg_0270_us\n",
      "Processing sample 70/152: zyc_seg_0077_us\n",
      "Processing sample 71/152: zyc_seg_0076_us\n",
      "Processing sample 72/152: syk_seg_0213_us\n",
      "Processing sample 73/152: db_seg_0191_us\n",
      "Processing sample 74/152: ldx_seg_0099_us\n",
      "Processing sample 75/152: syk_seg_0291_us\n",
      "Processing sample 76/152: syk_seg_0238_us\n",
      "Processing sample 77/152: syk_seg_0077_us\n",
      "Processing sample 78/152: pmz_seg_0176_us\n",
      "Processing sample 79/152: whh_seg_0186_us\n",
      "Processing sample 80/152: syk_seg_0237_us\n",
      "Processing sample 81/152: db_seg_0174_us\n",
      "Processing sample 82/152: syk_seg_0288_us\n",
      "Processing sample 83/152: zyc_seg_0055_us\n",
      "Processing sample 84/152: syk_seg_0112_us\n",
      "Processing sample 85/152: db_seg_0243_us\n",
      "Processing sample 86/152: ldx_seg_0155_us\n",
      "Processing sample 87/152: ldx_seg_0087_us\n",
      "Processing sample 88/152: db_seg_0188_us\n",
      "Processing sample 89/152: syk_seg_0191_us\n",
      "Processing sample 90/152: db_seg_0050_us\n",
      "Processing sample 91/152: pmz_seg_0075_us\n",
      "Processing sample 92/152: db_seg_0187_us\n",
      "Processing sample 93/152: db_seg_0054_us\n",
      "Processing sample 94/152: syk_seg_0256_us\n",
      "Processing sample 95/152: pmz_seg_0052_us\n",
      "Processing sample 96/152: pmz_seg_0018_us\n",
      "Processing sample 97/152: pmz_seg_0121_us\n",
      "Processing sample 98/152: whh_seg_0018_us\n",
      "Processing sample 99/152: pmz_seg_0099_us\n",
      "Processing sample 100/152: ldx_seg_0266_us\n",
      "Processing sample 101/152: db_seg_0276_us\n",
      "Processing sample 102/152: ldx_seg_0237_us\n",
      "Processing sample 103/152: db_seg_0058_us\n",
      "Processing sample 104/152: ldx_seg_0267_us\n",
      "Processing sample 105/152: pmz_seg_0098_us\n",
      "Processing sample 106/152: pmz_seg_0186_us\n",
      "Processing sample 107/152: syk_seg_0153_us\n",
      "Processing sample 108/152: ldx_seg_0196_us\n",
      "Processing sample 109/152: ldx_seg_0292_us\n",
      "Processing sample 110/152: whh_seg_0157_us\n",
      "Processing sample 111/152: db_seg_0121_us\n",
      "Processing sample 112/152: db_seg_0169_us\n",
      "Processing sample 113/152: pmz_seg_0143_us\n",
      "Processing sample 114/152: db_seg_0252_us\n",
      "Processing sample 115/152: db_seg_0235_us\n",
      "Processing sample 116/152: ldx_seg_0058_us\n",
      "Processing sample 117/152: pmz_seg_0150_us\n",
      "Processing sample 118/152: db_seg_0130_us\n",
      "Processing sample 119/152: ldx_seg_0021_us\n",
      "Processing sample 120/152: ldx_seg_0149_us\n",
      "Processing sample 121/152: syk_seg_0118_us\n",
      "Processing sample 122/152: db_seg_0214_us\n",
      "Processing sample 123/152: ldx_seg_0135_us\n",
      "Processing sample 124/152: ldx_seg_0047_us\n",
      "Processing sample 125/152: db_seg_0300_us\n",
      "Processing sample 126/152: syk_seg_0186_us\n",
      "Processing sample 127/152: whh_seg_0060_us\n",
      "Processing sample 128/152: ldx_seg_0067_us\n",
      "Processing sample 129/152: syk_seg_0002_us\n",
      "Processing sample 130/152: ldx_seg_0185_us\n",
      "Processing sample 131/152: whh_seg_0096_us\n",
      "Processing sample 132/152: pmz_seg_0038_us\n",
      "Processing sample 133/152: syk_seg_0007_us\n",
      "Processing sample 134/152: db_seg_0020_us\n",
      "Processing sample 135/152: db_seg_0071_us\n",
      "Processing sample 136/152: pmz_seg_0162_us\n",
      "Processing sample 137/152: whh_seg_0188_us\n",
      "Processing sample 138/152: db_seg_0160_us\n",
      "Processing sample 139/152: syk_seg_0226_us\n",
      "Processing sample 140/152: whh_seg_0116_us\n",
      "Processing sample 141/152: pmz_seg_0264_us\n",
      "Processing sample 142/152: whh_seg_0207_us\n",
      "Processing sample 143/152: syk_seg_0025_us\n",
      "Processing sample 144/152: syk_seg_0179_us\n",
      "Processing sample 145/152: pmz_seg_0288_us\n",
      "Processing sample 146/152: whh_seg_0135_us\n",
      "Processing sample 147/152: db_seg_0151_us\n",
      "Processing sample 148/152: db_seg_0099_us\n",
      "Processing sample 149/152: whh_seg_0162_us\n",
      "Processing sample 150/152: ldx_seg_0026_us\n",
      "Processing sample 151/152: ldx_seg_0232_us\n",
      "Processing sample 152/152: pmz_seg_0041_us\n"
     ]
    }
   ],
   "source": [
    "main_test_CrossViT()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
